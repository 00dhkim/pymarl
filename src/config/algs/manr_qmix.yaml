# --- QMIX specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000

## Conservative epsilon-greedy; start from 550k, duration: 50k, start: 0.5, finish: 0.05
# epsilon_start: 5.0
# epsilon_finish: 0.05
# epsilon_anneal_time: 550000

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
double_q: True
mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

name: "qmix"

# network randomization
runner: "random_episode"
mac: "manr_mac"
agent: "manr_rnn"
learner: "random_q_learner"

nr_alpha: 0.1 # network randomization (clean_flag) coef; 0 is fully clean path; 1.0 is fully random path
nr_beta: 0.002 # fm_loss coef
mc_approx: 10
uniform_matrix_start: 0.8
uniform_matrix_end: 1.2

# multi-agent network randomization, MANR
# agent마다 random layer를 달리 지정하여 observation의 randomness를 증가, 결과적으로 general한 모델을 학습하는 것이 목적.
# 